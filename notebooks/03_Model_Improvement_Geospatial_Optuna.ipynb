{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf0574ea-95ba-433a-8ae9-cedeb5e52264",
   "metadata": {},
   "source": [
    "# Model Improvement & Hyperparameter Optimization\n",
    "\n",
    "Enhancing the baseline model through geospatial feature engineering and automated hyperparameter tuning using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c1ac6b-f9b5-41da-b08c-a3c4585481d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /Users/alperen/miniconda3/lib/python3.13/site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/alperen/miniconda3/lib/python3.13/site-packages (from lightgbm) (2.3.3)\n",
      "Requirement already satisfied: scipy in /Users/alperen/miniconda3/lib/python3.13/site-packages (from lightgbm) (1.16.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "446d613a-14f9-4b99-ac05-db22b9b7bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da24d1c6-aa72-42ec-8852-ea8790ba4c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading team's processed data...\n"
     ]
    }
   ],
   "source": [
    "# LOAD TEAM'S PROCESSED DATA\n",
    "\n",
    "print(\"Loading team's processed data...\")\n",
    "\n",
    "# Loading artifacts created by teammates\n",
    "X_team = joblib.load(\"../data/X.joblib\")\n",
    "y = joblib.load(\"../data/y.joblib\")\n",
    "test_ids = joblib.load(\"../data/test_ids.joblib\")\n",
    "X_test_team = joblib.load(\"../data/X_test.joblib\")\n",
    "\n",
    "# Load raw data for geospatial calculations (Latitude/Longitude)\n",
    "train_raw = pd.read_csv(\"../data/train.csv\")\n",
    "test_raw = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# Align raw data with processed data (Filtering outliers as done by the team)\n",
    "train_raw[\"price\"] = train_raw[\"price\"].replace({\"\\$\": \"\", \",\": \"\"}, regex=True).astype(float)\n",
    "train_raw = train_raw.dropna(subset=[\"price\"])\n",
    "q_high = train_raw[\"price\"].quantile(0.99)\n",
    "train_filtered = train_raw[train_raw[\"price\"] <= q_high].reset_index(drop=True)\n",
    "\n",
    "# Ensure row counts match\n",
    "if train_filtered.shape[0] != X_team.shape[0]:\n",
    "    train_filtered = train_filtered.iloc[:X_team.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b5d5d3a-3261-4448-8324-57a40adcef9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating geospatial features...\n"
     ]
    }
   ],
   "source": [
    "# GEOSPATIAL FEATURE ENGINEERING\n",
    "\n",
    "print(\"Generating geospatial features...\")\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    #Calculates distance (km) between two coordinates.\n",
    "    R = 6371  # Earth radius\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi/2)**2 + np.cos(phi1)*np.cos(phi2)*np.sin(dlambda/2)**2\n",
    "    c = 2*np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "# Critical locations in Istanbul impacting price\n",
    "locations = {\n",
    "    \"Taksim\": (41.0370, 28.9851),\n",
    "    \"Sultanahmet\": (41.0054, 28.9768),\n",
    "    \"Besiktas\": (41.0422, 29.0060),\n",
    "    \"Kadikoy\": (40.9901, 29.0254),\n",
    "    \"Airport\": (41.2811, 28.7533)\n",
    "}\n",
    "\n",
    "def get_geo_features(df):\n",
    "    geo_data = pd.DataFrame()\n",
    "    for loc, (lat, lon) in locations.items():\n",
    "        geo_data[f\"dist_{loc}\"] = haversine_distance(df[\"latitude\"], df[\"longitude\"], lat, lon)\n",
    "    \n",
    "    # Distance to the nearest city center (excluding airport)\n",
    "    centers = [c for c in geo_data.columns if \"Airport\" not in c]\n",
    "    geo_data[\"min_dist_center\"] = geo_data[centers].min(axis=1)\n",
    "    return geo_data\n",
    "    \n",
    "# Generate features\n",
    "X_geo_train = get_geo_features(train_filtered)\n",
    "X_geo_test = get_geo_features(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1b1247c-2ca7-426a-b767-41432ecc0f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging features...\n"
     ]
    }
   ],
   "source": [
    "# MERGE & CLEAN\n",
    "\n",
    "print(\"Merging features...\")\n",
    "# Combine Team's Features + New Geo Features\n",
    "X_final = pd.concat([X_team.reset_index(drop=True), X_geo_train.reset_index(drop=True)], axis=1)\n",
    "X_test_final = pd.concat([X_test_team.reset_index(drop=True), X_geo_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Clean column names to prevent LightGBM errors (special characters/duplicates)\n",
    "def clean_col_names(df):\n",
    "    new_cols = []\n",
    "    seen_cols = {}\n",
    "    for col in df.columns:\n",
    "        new_col = re.sub(r'[^A-Za-z0-9_]+', '', str(col))\n",
    "        if new_col in seen_cols:\n",
    "            seen_cols[new_col] += 1\n",
    "            new_col = f\"{new_col}_{seen_cols[new_col]}\"\n",
    "        else:\n",
    "            seen_cols[new_col] = 1\n",
    "        new_cols.append(new_col)\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "X_final = clean_col_names(X_final)\n",
    "X_test_final = clean_col_names(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f770e878-a845-468d-8134-2e35d849535c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /Users/alperen/miniconda3/lib/python3.13/site-packages (4.6.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/alperen/miniconda3/lib/python3.13/site-packages (from optuna) (1.17.2)\n",
      "Requirement already satisfied: colorlog in /Users/alperen/miniconda3/lib/python3.13/site-packages (from optuna) (6.10.1)\n",
      "Requirement already satisfied: numpy in /Users/alperen/miniconda3/lib/python3.13/site-packages (from optuna) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alperen/miniconda3/lib/python3.13/site-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/alperen/miniconda3/lib/python3.13/site-packages (from optuna) (2.0.44)\n",
      "Requirement already satisfied: tqdm in /Users/alperen/miniconda3/lib/python3.13/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /Users/alperen/miniconda3/lib/python3.13/site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /Users/alperen/miniconda3/lib/python3.13/site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /Users/alperen/miniconda3/lib/python3.13/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/alperen/miniconda3/lib/python3.13/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc721a01-8b79-49e5-b96a-bc500c8aa1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 02:38:11,510] A new study created in memory with name: no-name-d3b461c8-43d4-4bcc-8c78-ef865f1d636f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 02:38:17,356] Trial 0 finished with value: 0.42467776374313393 and parameters: {'n_estimators': 691, 'learning_rate': 0.036459672832288126, 'num_leaves': 89, 'max_depth': 8, 'colsample_bytree': 0.858742793555864, 'subsample': 0.5343167316151838}. Best is trial 0 with value: 0.42467776374313393.\n",
      "[I 2025-12-02 02:38:20,280] Trial 1 finished with value: 0.4224744719330957 and parameters: {'n_estimators': 1217, 'learning_rate': 0.0914301961837681, 'num_leaves': 25, 'max_depth': 8, 'colsample_bytree': 0.8565901402587315, 'subsample': 0.883577982435841}. Best is trial 1 with value: 0.4224744719330957.\n",
      "[I 2025-12-02 02:38:23,787] Trial 2 finished with value: 0.418610255341611 and parameters: {'n_estimators': 1451, 'learning_rate': 0.08236359401967983, 'num_leaves': 24, 'max_depth': 7, 'colsample_bytree': 0.58728582603724, 'subsample': 0.8498208352647839}. Best is trial 2 with value: 0.418610255341611.\n",
      "[I 2025-12-02 02:38:31,918] Trial 3 finished with value: 0.41759240089351135 and parameters: {'n_estimators': 1956, 'learning_rate': 0.03729195220256262, 'num_leaves': 71, 'max_depth': 7, 'colsample_bytree': 0.6233224343060486, 'subsample': 0.9973414972272727}. Best is trial 3 with value: 0.41759240089351135.\n",
      "[I 2025-12-02 02:38:37,958] Trial 4 finished with value: 0.4172925303158825 and parameters: {'n_estimators': 2462, 'learning_rate': 0.056592478085349104, 'num_leaves': 33, 'max_depth': 8, 'colsample_bytree': 0.8055637061134744, 'subsample': 0.518703908694254}. Best is trial 4 with value: 0.4172925303158825.\n",
      "[I 2025-12-02 02:38:42,676] Trial 5 finished with value: 0.4232300523624998 and parameters: {'n_estimators': 1200, 'learning_rate': 0.07814836030330169, 'num_leaves': 46, 'max_depth': 13, 'colsample_bytree': 0.8645496940049964, 'subsample': 0.7772322955253799}. Best is trial 4 with value: 0.4172925303158825.\n",
      "[I 2025-12-02 02:38:58,936] Trial 6 finished with value: 0.4228384167725156 and parameters: {'n_estimators': 1526, 'learning_rate': 0.017898658913650445, 'num_leaves': 95, 'max_depth': 9, 'colsample_bytree': 0.8559706905230663, 'subsample': 0.5476262945593622}. Best is trial 4 with value: 0.4172925303158825.\n",
      "[I 2025-12-02 02:39:07,976] Trial 7 finished with value: 0.4194756541397466 and parameters: {'n_estimators': 1958, 'learning_rate': 0.09094515220061138, 'num_leaves': 88, 'max_depth': 12, 'colsample_bytree': 0.5168363752511111, 'subsample': 0.8494002590354723}. Best is trial 4 with value: 0.4172925303158825.\n",
      "[I 2025-12-02 02:39:14,236] Trial 8 finished with value: 0.4187487498122786 and parameters: {'n_estimators': 1411, 'learning_rate': 0.06663245600543088, 'num_leaves': 100, 'max_depth': 8, 'colsample_bytree': 0.5634202067996443, 'subsample': 0.8236094053823055}. Best is trial 4 with value: 0.4172925303158825.\n",
      "[I 2025-12-02 02:39:16,550] Trial 9 finished with value: 0.42416179241445484 and parameters: {'n_estimators': 1220, 'learning_rate': 0.08377546579485184, 'num_leaves': 20, 'max_depth': 9, 'colsample_bytree': 0.7156327409906335, 'subsample': 0.7585339499794967}. Best is trial 4 with value: 0.4172925303158825.\n",
      "[I 2025-12-02 02:39:20,560] Trial 10 finished with value: 0.42593812770317263 and parameters: {'n_estimators': 2463, 'learning_rate': 0.052622495819970756, 'num_leaves': 44, 'max_depth': 5, 'colsample_bytree': 0.9881459562677405, 'subsample': 0.644204059602679}. Best is trial 4 with value: 0.4172925303158825.\n",
      "[I 2025-12-02 02:39:26,757] Trial 11 finished with value: 0.4223644626798241 and parameters: {'n_estimators': 2462, 'learning_rate': 0.04270463728651544, 'num_leaves': 72, 'max_depth': 5, 'colsample_bytree': 0.6888775414377851, 'subsample': 0.9980170418311479}. Best is trial 4 with value: 0.4172925303158825.\n",
      "[I 2025-12-02 02:39:43,469] Trial 12 finished with value: 0.4157149792805537 and parameters: {'n_estimators': 2051, 'learning_rate': 0.024427599845727344, 'num_leaves': 69, 'max_depth': 11, 'colsample_bytree': 0.6530593230226798, 'subsample': 0.6544158061512633}. Best is trial 12 with value: 0.4157149792805537.\n",
      "[I 2025-12-02 02:39:57,735] Trial 13 finished with value: 0.41955388978887376 and parameters: {'n_estimators': 2148, 'learning_rate': 0.013040387029077722, 'num_leaves': 52, 'max_depth': 15, 'colsample_bytree': 0.7693605055121842, 'subsample': 0.6379162375947155}. Best is trial 12 with value: 0.4157149792805537.\n",
      "[I 2025-12-02 02:40:13,126] Trial 14 finished with value: 0.4152642890434576 and parameters: {'n_estimators': 2189, 'learning_rate': 0.02407423604191973, 'num_leaves': 68, 'max_depth': 11, 'colsample_bytree': 0.7786920965397112, 'subsample': 0.6452699024537}. Best is trial 14 with value: 0.4152642890434576.\n",
      "[I 2025-12-02 02:40:27,322] Trial 15 finished with value: 0.4148289623378424 and parameters: {'n_estimators': 2059, 'learning_rate': 0.025343452969683825, 'num_leaves': 68, 'max_depth': 11, 'colsample_bytree': 0.6625893951602744, 'subsample': 0.6509179025430295}. Best is trial 15 with value: 0.4148289623378424.\n",
      "[I 2025-12-02 02:40:40,636] Trial 16 finished with value: 0.4153070578748439 and parameters: {'n_estimators': 1802, 'learning_rate': 0.025901558092115952, 'num_leaves': 60, 'max_depth': 14, 'colsample_bytree': 0.7485653637381655, 'subsample': 0.6967116017544055}. Best is trial 15 with value: 0.4148289623378424.\n",
      "[I 2025-12-02 02:40:53,383] Trial 17 finished with value: 0.42017789780424425 and parameters: {'n_estimators': 1783, 'learning_rate': 0.028713026836376162, 'num_leaves': 79, 'max_depth': 11, 'colsample_bytree': 0.9265669113458033, 'subsample': 0.6011286650180444}. Best is trial 15 with value: 0.4148289623378424.\n",
      "[I 2025-12-02 02:41:08,846] Trial 18 finished with value: 0.4197189132503243 and parameters: {'n_estimators': 2181, 'learning_rate': 0.01115704162857499, 'num_leaves': 58, 'max_depth': 11, 'colsample_bytree': 0.6765184287112298, 'subsample': 0.7110481593531846}. Best is trial 15 with value: 0.4148289623378424.\n",
      "[I 2025-12-02 02:41:16,834] Trial 19 finished with value: 0.4171332268895475 and parameters: {'n_estimators': 2264, 'learning_rate': 0.05061447685963126, 'num_leaves': 62, 'max_depth': 13, 'colsample_bytree': 0.7918208910253812, 'subsample': 0.5701288973010334}. Best is trial 15 with value: 0.4148289623378424.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE: 0.41483\n",
      "Best Params: {'n_estimators': 2059, 'learning_rate': 0.025343452969683825, 'num_leaves': 68, 'max_depth': 11, 'colsample_bytree': 0.6625893951602744, 'subsample': 0.6509179025430295}\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETER OPTIMIZATION (OPTUNA)\n",
    "import optuna\n",
    "print(\"Starting Optuna optimization...\")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_final, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 2500),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 100),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 15),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_valid)\n",
    "    rmse = np.sqrt(mean_squared_error(y_valid, preds))\n",
    "    return rmse\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20) # Keep trials low for quick demo, increase for better results\n",
    "\n",
    "print(f\"Best RMSE: {study.best_value:.5f}\")\n",
    "print(\"Best Params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cf4a538-a962-40d2-8ac0-6f9aa5967958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final model with best parameters...\n",
      "Saving artifacts...\n",
      "Model saved and submission created successfully.\n"
     ]
    }
   ],
   "source": [
    "# TRAIN & SAVE FINAL MODEL\n",
    "import os\n",
    "\n",
    "print(\"Training final model with best parameters...\")\n",
    "best_model = lgb.LGBMRegressor(**study.best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "os.makedirs(\"../data\", exist_ok=True)\n",
    "os.makedirs(\"../submissions\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Save artifacts for the Evaluation Notebook\n",
    "print(\"Saving artifacts...\")\n",
    "joblib.dump(best_model, \"../models/best_lgbm_model.pkl\")\n",
    "joblib.dump(X_final, \"../data/X_final_enhanced.joblib\")\n",
    "joblib.dump(y, \"../data/y_final.joblib\") \n",
    "joblib.dump(X_valid, \"../data/X_valid_for_eval.joblib\") \n",
    "joblib.dump(y_valid, \"../data/y_valid_for_eval.joblib\")\n",
    "\n",
    "# Generate Submission\n",
    "final_preds = np.expm1(best_model.predict(X_test_final))\n",
    "final_preds = np.maximum(final_preds, 0)\n",
    "\n",
    "# Use original IDs\n",
    "df_test_orig = pd.read_csv(\"../data/test.csv\")\n",
    "sub = pd.DataFrame({\"ID\": df_test_orig[\"id\"], \"TARGET\": final_preds})\n",
    "sub.to_csv(f\"../submissions/submission_optuna_{study.best_value:.4f}.csv\", index=False)\n",
    "\n",
    "print(\"Model saved and submission created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afa8fe1-f6ab-4cf9-bb87-5355d9814d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fff598-cde0-49b1-a57b-7c8f8e2d6800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
